## Created: June 9, 2021
## Updated: January 31, 2023  
## Bianca Luedeker
## Analyzing the water maze data in four ways
## The end of the script has to do with the mouse water maze paer.

#### Packages ####
library(compositions)  ##analysis of compositional data from book
library(progress)      ## Creates progress bars for looped functions.
library(xtable)        ## exports tables and matrices in latex format.
library(sirt)          ##Estimates Dirichlet alpha parameters
library(gtools)  ##Generates all possible permutations, rdirichlet here
library(DirichletReg)  ## Compare two Dirichlet Models
library(rje)
library(matlib)      ## Easy inverse of a matrix.


#### Reading in Data/ Preliminary An=alysis ####

water.maze.data <- read.csv("water_maze_data.csv")
# View(water.maze.data)

tg.data <- as.matrix(water.maze.data[1:7, 2:5])
wild.data <- as.matrix(water.maze.data[8:14, 2:5])

## All correlations are negative for the TG group.
cov(tg.data)
cor(tg.data)
# xtable(cor(tg.data))

## The correlation between OQ and AQ1 is positive!  This is evidence a Dirichlet
## is not the correct model for the wild rodents.
cov(wild.data)
cor(wild.data)
# xtable(cor(wild.data))

## If the data set is combined into one set, then the positive correlation persists
## between AQ1 and OQ.  This should be in the same nested part of a tree.
full.data <- as.matrix(water.maze.data[, 2:5])
cov(full.data)
cor(full.data)
# xtable(cor(full.data))

## Problem:  In one group, OQ and AQ1 are negatively correlated and in the other
## group they are positively correlated.  How can we mitigate this in the nesting structure?

#### Aitchison's Distance and Permutation Test ####
## Aitchison's Distance between y11 and y12.  AIT(y11, y12)

clr.full.data <- compositions::clr(full.data)

## Function for computing the test statistic.  Impute the two group data sets
## that have already been clr transformed.
ait.test.stat <- function(group.one, group.two){
  test.stat <- 0
  for (h in 1:7){
    for (i in 1:7){
      aa <- (sum((group.one[h,] - group.two[i,])^2))^0.5
      test.stat <- test.stat + aa
    }
  }
  return(test.stat)
}

## Test statistic for actual data.

observed.stat <- ait.test.stat(clr.full.data[1:7,], clr.full.data[8:14,])

## Create the distribution of the test statistic using all 3432 permutations.

all.ait.test.stats <- function() {
  index.g1 <- t(combn(1:14, 7))
  all.test.stats <- NULL
  pb <- progress_bar$new(total = 3432)
  for(i in 1:3432){
    aa <- index.g1[i,]
    group.one <- clr.full.data[aa,]
    group.two <- clr.full.data[-aa,]
    all.test.stats <- c(all.test.stats, ait.test.stat(group.one, group.two))
    pb$tick()
    Sys.sleep(1 / 3432)
  }
  return(all.test.stats)
}

all.ait <- all.ait.test.stats()
# write.csv(all.ait, file="AIT_Test_Stats.csv")

# all.ait <- read.csv("AIT_Test_Stats.csv")[,2]
hist(all.ait, main = "All Possible Values of the AIT Statistic", xlab="AIT Distance")
abline(v=observed.stat, col="dark red", lwd=4)
text(x=observed.stat, y= 600, "Observed")

## This gives a p-value of 0.170
sum(all.ait >= observed.stat)/3432
  
  
#### Analysis 2: Using LRT as in Maugard Paper ####

##Parameter estimates under H1.
sirt::dirichlet.mle(tg.data)
sirt::dirichlet.mle(wild.data)

## Parameter Estimates under H0.  I don't think they need to use the techniques
## they say they need to since this is a univariate function.

## Plot the log-likelihood function to look for a max

log.like.null <- function(alpha, dat){
  sum.p.bar <- sum(apply(log(dat), 2, mean))
  result <- 7*log(gamma(4*alpha)) - 28*log(gamma(alpha)) + 7*(alpha-1)*sum.p.bar
  return(result)
}

alpha <- seq(0.1, 30, by=0.1)
y.tg <- log.like.null(alpha, tg.data)
y.wild <- log.like.null(alpha, wild.data)

par(mfrow=c(1, 2))
plot(alpha, y.tg, type="l", col="blue", ylab = "log-likelihood", main="TG Group")
plot(alpha, y.wild, type="l", col="blue", ylab = "log-likelihood", main="Wild Group")
title("Null Log-Likelihood", outer=TRUE, line=-1)

## Take the derivative of the log-likelihood.
deriv.null.log.like <- function(alpha, data){
  sum.p.bar<- sum(apply(log(data), 2, mean))
  result <- 28*digamma(4*alpha) - 28*digamma(alpha) + 7*sum.p.bar
  return(result)
}

alpha <- seq(6.5, 10.5,  by=0.1)
y.tg <- deriv.null.log.like(alpha, tg.data)
par(mfrow=c(1,2))
plot(alpha, y.tg, type="l", col="blue", ylab = "log-likelihood derivative", main="TG Group")
abline(h=0, col="red")
alpha <- seq(2, 7, by=0.1)
y.wild <- deriv.null.log.like(alpha, wild.data)
plot(alpha, y.wild, type="l", col="blue", ylab = "log-likelihood derivative", main="Wild Group")
abline(h=0, col="red")
title("Derivatives of Null Log-Likelihood", outer=TRUE, line=-1)


## Use Uniroot to find the zeros.
## Uniroot can only handle a function of one variable.

deriv.null.tg <- function(alpha) deriv.null.log.like(alpha, tg.data)
deriv.null.wild <- function(alpha) deriv.null.log.like(alpha, wild.data)

#where the maximums occur
max.alpha.tg.null <- uniroot(deriv.null.tg, c(8,9))$root  ##8.482
max.alpha.wild.null <- uniroot(deriv.null.wild, c(2, 4))$root #3.029

#The maximums under the null.
max.null.tg <- log.like.null(max.alpha.tg.null, tg.data)  ##27.3675
max.null.wild <- log.like.null(max.alpha.wild.null, wild.data)   ##18.071

## Where the maxs occur with no restrictions (alternative hypothesis)
max.alpha.tg.alt <- sirt::dirichlet.mle(tg.data)$alpha
max.alpha.wild.alt <- sirt::dirichlet.mle(wild.data)$alpha

## Create log-likelihood function for four distinct alphas.
log.like.alt <- function(alphas, dat) {
  log.p.bar <- apply(log(dat), 2, mean)
  sum.alpha <- sum(alphas)
  result <- 7*log(gamma(sum.alpha)) - 7*sum(log(gamma(alphas))) + 7*sum((alphas-1)*log.p.bar)
  return(result)
}

## The maximums under the alternative.
(max.alt.tg <- log.like.alt(max.alpha.tg.alt, tg.data))  ##29.62586
(max.alt.wild <- log.like.alt(max.alpha.wild.alt, wild.data))  ## 26.38417

## The values of the LRT statistic.
(LR.tg <- 2*(max.alt.tg - max.null.tg)) ##4.516735
(LR.wild <- 2*(max.alt.wild - max.null.wild))  ##16.62658

## P-values.  The LRT is approximately chi-squared with df = 3.
pchisq(LR.tg, 3, lower.tail = FALSE)  # 0.211
pchisq(LR.wild, 3, lower.tail = FALSE)  # 0.001

## With the small sample size correction.
(ss.correction <- 3/(3+5.9*7^-1.4))  # 0.886
(LR.tilde.tg <- ss.correction*LR.tg)  # 4.001
(LR.tilde.wild <- ss.correction*LR.wild)  # 14.727

## p-values with the small sample size correction.

pchisq(LR.tilde.tg, 3, lower.tail = FALSE)  # 0.261
pchisq(LR.tilde.wild, 3, lower.tail = FALSE)  # 0.0021


#### Analysis 3: Use Aitchison as Presented in ACD with R book  ####

## Step 1: Give the compostion the class acomp (Aitchison Composition) to apply
## the functions in the composition package.

water.maze.data <- read.csv("water_maze_data.csv")
YY <- acomp(water.maze.data[,2:5])  ##The composition is the dependent variable YY.
XX <- factor(water.maze.data[,1], ordered=FALSE)  ##The only covariable: group membership.

plot(YY, pch=20, col = c("red", "blue")[XX], main="3 Part Subcompositions for 3tg and Wild Groups")
# legend(locator(1), levels(XX), pch=20, col=c("red", "blue"), xpd=NA,  yjust=0)

## The linear Model
(two.group.lm <- lm(ilr(YY)~XX))
summary(two.group.lm)  ## This is not helpful for compositions and should be ignored.

setStickyClassOption(FALSE) ## This command lets you use the properties of an earlier version of compositions.
anova(two.group.lm)  ##ANOVA only works with version 1 of compositions.

##Export as latex.
# xtable(anova(two.group.lm))
## We get a p-value of 0.242315.  This means that the slope coefficient for groups should not be included.
setStickyClassOption(TRUE)  ## This command lets you go back to the new version.


## Analysis 4: As a non-nested Dirichlet.

YY <- DR_data(water.maze.data[,2:5])
XX <- factor(water.maze.data[,1])

mod1 <- DirichletReg::DirichReg(YY ~ XX, model="common")
summary(mod1)

mod2 <- DirichletReg::DirichReg(YY ~ 1, model = "common")
summary(mod2)

anova(mod1, mod2)  ## p-value of 0.0691.  Use the simpler model.

## Analysis 5: As a nested Dirichlet Tree
## There are 12 possible binary cascade trees that can be made.  Which out of 
## the 12 is the best fit for the data?  Which of the 12 has the same
## positive negative correlation as the sample data?

## Try one of the 12 trees to get an idea on how to go through all possible 12 
## trees.

## Example 1: [(TQ), (OQ, AQ1, AQ2)] --> [(AQ1), (OQ, AQ2)] --> [(OQ), (AQ2)]

## Estimate Parameters for first tree.

full.data <- water.maze.data[, 2:5]
names(full.data) <- c("TQ", "OQ", "A1", "A2") ##slightly shorter names for ease of use.
L1 <- cbind(full.data[,1], other=rowSums(full.data[,2:4]))
L1.alpha <- sirt::dirichlet.mle(L1)$alpha
L1.totals <- rowSums(full.data[,2:4])
L2 <- cbind(A1=full.data[,3]/L1.totals, other=rowSums(full.data[,c(2,4)])/L1.totals)
L2.alpha <- sirt::dirichlet.mle(L2)$alpha
L2.totals <- rowSums(full.data[,c(2,4)])                   
L3 <- cbind(OQ = full.data[,2]/L2.totals, A2 = full.data[,4]/L2.totals)
L3.alpha <- sirt::dirichlet.mle(L3)$alpha

## Create a data set using these parameters.
set.seed(1983)
L1.data <- rdirichlet(500, L1.alpha)
L2.data <- rdirichlet(500, L2.alpha)
L3.data <- rdirichlet(500, L3.alpha)
TQ.sim <- L1.data[,1]
A1.sim <- L1.data[,2]*L2.data[,1]
OQ.sim <- L1.data[,2]*L2.data[,2]*L3.data[,1]
A2.sim <-L1.data[,2]*L2.data[,2]*L3.data[,2]
sim.data <- cbind(TQ.sim, OQ.sim, A1.sim, A2.sim)
cor(sim.data)

##This still gives negative only correlation is that because positive correlation 
## only occurs when parts are on the same level in a tree?

##Check using the example from thesis:

L1.data <- rdirichlet(500, c(2, 8))
L2.data <- rdirichlet(500, c(20, 20, 20, 20))
sim.data <- cbind(L1.data[,1], L1.data[,2]*L2.data[,1], L1.data[,2]*L2.data[,2],
                L1.data[,2]*L2.data[,3], L1.data[,2]*L2.data[,4])

## Try again with the setup {(TQ, A1), (A2), (OQ)}

full.data <- water.maze.data[, 2:5]
names(full.data) <- c("TQ", "OQ", "A1", "A2") ##slightly shorter names for ease of use.
L1 <- cbind("OQ"=full.data[,2], "A2"=full.data[,4], "TQ+A1"=rowSums(full.data[, c(1, 3)]))
L1.alpha <- sirt::dirichlet.mle(L1)$alpha
L1.totals <- rowSums(full.data[,c(1, 3)])
L2 <- cbind("TQ*"=full.data[,1]/L1.totals, "A1*"=full.data[,3]/L1.totals)
L2.alpha <- sirt::dirichlet.mle(L2)$alpha

##Create a simulated data set using these alpha values.
L1.data <- rdirichlet(500, L1.alpha)
L2.data <- rdirichlet(500, L2.alpha)
OQsim <- L1.data[,1]
A2sim <- L1.data[,2]
TQsim <- L1.data[,3]*L2.data[,1]
A1sim <- L1.data[,3]*L2.data[,2]
sim.data <- cbind(TQsim, OQsim, A1sim, A2sim)
cor(sim.data)

L1.data <- rdirichlet(500, c(6, 6, 15))
L2.data <- rdirichlet(500, c(40, 20))
OQsim <- L1.data[,1]
A2sim <- L1.data[,2]
TQsim <- L1.data[,3]*L2.data[,1]
A1sim <- L1.data[,3]*L2.data[,2]
sim.data <- cbind(TQsim, OQsim, A1sim, A2sim)
cor(sim.data)


#### Part 2:  Simulation Study using the Author's approach ####
## What is the probability of a type I error and type II error of comparing
## both groups to a standard then deciding if they are the same or not based 
## on whether we fail to reject both or fail to reject on one and reject on another.
## Particular we are interested in Type I error since that is what I claim they 
## committed in the paper.

####  Part 2-A: type one error ####
## Use estimates of alpha parameters based on the data seen in this example.
## Last Updated: June 30, 2021


##Necessary functions (Re pasted from above)

## Log-likelihood function for four distinct alphas seen in alternative Hypothesis.
log.like.alt <- function(alphas, dat) {
  log.p.bar <- apply(log(dat), 2, mean)
  sum.alpha <- sum(alphas)
  result <- 7*log(gamma(sum.alpha)) - 7*sum(log(gamma(alphas))) + 7*sum((alphas-1)*log.p.bar)
  return(result)
}

## Log-likelihood function for the null of equal alphas.
log.like.null <- function(alpha, dat){
  sum.p.bar <- sum(apply(log(dat), 2, mean))
  result <- 7*log(gamma(4*alpha)) - 28*log(gamma(alpha)) + 7*(alpha-1)*sum.p.bar
  return(result)
}

## Derivative of null log-likelihood
deriv.null.log.like <- function(alpha, data){
  sum.p.bar<- sum(apply(log(data), 2, mean))
  result <- 28*digamma(4*alpha) - 28*digamma(alpha) + 7*sum.p.bar
  return(result)
}


## Water Maze Simulation Study Function.
## Input a vector of Dirichlet Parameters alpha and number of simulations n.
water.maze.sim <- function(n, alphas){
  sim.data <- array(NA, dim=c(14, 4)) ## Matrix to hold the raw simulated data.
  reject.both <- 0
  accept.both <- 0
  not.the.same <- 0
  pb <- progress_bar$new(total = n)
  for(i in 1:n){
    ##Create data and divide into two groups.
    sim.data <- rdirichlet(14, alphas)
    tg.data <- sim.data[1:7,]
    wild.data <- sim.data[8:14,]
  
    ## Looking for MLES under the NUll
    deriv.null.tg <- function(alpha) deriv.null.log.like(alpha, tg.data)
    deriv.null.wild <- function(alpha) deriv.null.log.like(alpha, wild.data)
    #where the maximums occur under the NULL
    ifelse (sign(deriv.null.tg(0.1))==sign(deriv.null.tg(100)), max.alpha.tg.null <- NA,
            max.alpha.tg.null <- uniroot(deriv.null.tg, c(0.1,100))$root )
    ifelse(sign(deriv.null.wild(0.1))==sign(deriv.null.wild(100)), max.alpha.wild.null <- NA,
          max.alpha.wild.null <- uniroot(deriv.null.wild, c(0.1, 100))$root )
    ## Maximums under the null.
    max.tg.null <- log.like.null(max.alpha.tg.null, tg.data)
    max.wild.null <- log.like.null(max.alpha.wild.null, wild.data)
  
    # Parameter MLES  under the alternative.
    ## Where the maxs occur with no restrictions (alternative hypothesis)
    max.alpha.tg.alt <- sirt::dirichlet.mle(tg.data)$alpha
    max.alpha.wild.alt <- sirt::dirichlet.mle(wild.data)$alpha
    ## Maximums under the alternative.
    max.tg.alt <- log.like.alt(max.alpha.tg.alt, tg.data)
    max.wild.alt <- log.like.alt(max.alpha.wild.alt, wild.data)
    
    
    ss.correction <- 3/(3+5.4*7^-1.4)  ##small sample size correction.
    LR.stat.tg <- 2*(max.tg.alt - max.tg.null)*ss.correction
    p.tg <- pchisq(LR.stat.tg, 3, lower.tail=FALSE)
    LR.stat.wild <- 2*(max.wild.alt - max.wild.null)*ss.correction
    p.wild <- pchisq(LR.stat.wild, 3, lower.tail=FALSE)
  
    if(p.tg < 0.05 & p.wild < 0.05){ reject.both <- reject.both + 1}
    else if(p.tg >= 0.05 & p.wild >= 0.05){ accept.both <- accept.both + 1}
    else {not.the.same <- not.the.same + 1 }

    results <- c(reject.both, accept.both, not.the.same)

    pb$tick()
    Sys.sleep(1 / n)
  }
return(results)
}

## Different Simulation Settings.

water.maze.data <- read.csv("water_maze_data.csv")
(data.alphas<- sirt::dirichlet.mle(water.maze.data[,2:5])$alpha) ##Estimate appropriate alpha values using sample data.

sim.alphas.1 <- c(9.8, 6.1, 5.4, 5.9)  ## Rounded values close to those seen in the data.  Precision 27.2
sim.alphas.2 <- c(6.8, 6.8, 6.8, 6.8 ) ## Uniform, precision 27.2
sim.alphas.3 <- c(9, 6, 6, 6)          ## Rounded similar to data
sim.alphas.4 <- c(8, 7, 7, 5)
sim.alphas.5 <- c(12, 5, 5, 5)  ## Most extreme in one value

set.seed(1983)
(water.maze.sim(10000, sim.alphas.1))  ## (5501, 663, 3836) (reject both, accept both, reject one/accept one)
set.seed(1983)
(water.maze.sim(10000, sim.alphas.2))  ## (26, 9033, 941) 
#20,000 single tests, reject = 26*2+941 = 993, type one = 993/20000 = 0.04965)
set.seed(1983)
(water.maze.sim(10000, sim.alphas.3))  ##(2307, 2675, 5018)
set.seed(1983)
(water.maze.sim(10000, sim.alphas.4))  ##(1812, 3269, 4919)
set.seed(1983)
(water.maze.sim(10000, sim.alphas.5))  ##(9918, 0, 82)

all.sim.results <- data.frame( 
  "alpha" = c(9.8, 6.8, 9, 8, 12), 
  "reject both" = c(5501, 26, 2307, 1812, 9918),
  "fail to reject both" = c(663, 9033, 2675, 3269, 0),
   "reject only one" = c(3836, 941, 5018, 4919, 82), 
   "P(type I error" = c(0.3836, 0.0941, 0.5018, 0.4919, 0.0082))
xtable(all.sim.results)

## July 6th, 20 nested tree designs

## Example 1:  Nested Design {{TQ, A1}, {OQ, A2}}

full.data <- water.maze.data[, 2:5]
names(full.data) <- c("TQ", "OQ", "A1", "A2") ##slightly shorter names for ease of use.

## Create different 2 part subcompositions for each sub-tree

b1 <- full.data$TQ + full.data$A1
b2 <- full.data$OQ + full.data$A2
b11 <- full.data$TQ/b1
b12 <- full.data$A1/b1
b21 <- full.data$OQ/b2
b22 <- full.data$A2/b2

nest.data <-list(level.1 = cbind(b1, b2), level.2=cbind(b11, b12), level.3=cbind(b21, b22))
(top.level.mle <- sirt::dirichlet.mle(nest.data[[1]])$alpha)
(bot.level.L.mle <- sirt::dirichlet.mle(nest.data[[2]])$alpha)
(bot.level.R.mle <- sirt::dirichlet.mle(nest.data[[3]])$alpha)

alphas <- list(top.level.mle, bot.level.L.mle, bot.level.R.mle)

my.log.like.fun <- function(alpha, data){
  n <- nrow(data)
  log.p.bar <- apply(log(data), 2, mean)
  n*log(gamma(sum(alpha))) - n*sum(log(gamma(alpha))) + n*sum((alpha-1)*log.p.bar)
}

 -2*(my.log.like.fun(alphas[[1]], nest.data[[1]]) +my.log.like.fun(alphas[[2]], nest.data[[2]]) +
my.log.like.fun(alphas[[3]], nest.data[[3]]))  ## -62.97717

## Try with no splits.

(full.alphas <- sirt::dirichlet.mle(full.data)$alpha)
-2*my.log.like.fun(full.alphas, full.data)


## All 20 trees

criteria <- rep(0, 20)

## Type 1: No Splits. (1)

(full.alphas <- sirt::dirichlet.mle(full.data)$alpha)
criteria[1] <- -2*my.log.like.fun(full.alphas, full.data)

## Type 2: 1 vs 3 (4)
AA <- combn(1:4, 3)
for(j in 1:4){
  trip <- AA[,j]
  b1 <- rowSums(full.data[,trip])
  b2 <- 1 - b1
  bot.level <- full.data[,trip]/b1
  (top.mle <- sirt::dirichlet.mle(cbind(b1, b2))$alpha)
  (bot.mle <- sirt::dirichlet.mle(bot.level)$alpha)
  criteria[j+1] <- -2*(my.log.like.fun(top.mle, cbind(b1, b2)) + 
                         my.log.like.fun(bot.mle, bot.level))
  }

## Type 3: 2 v 2 (3)
AA <- combn(1:4, 2)
for(j in 1:3){
  duo <- AA[, j]
  b1 <- rowSums(full.data[,duo])
  b2 <- 1 - b1
  top <- cbind(b1, b2)
  botL <- full.data[,duo]/b1
  botR <- full.data[, -duo]/b2
  top.mle <- sirt::dirichlet.mle(top)$alpha
  botL.mle <- sirt::dirichlet.mle(botL)$alpha
  botR.mle <- sirt::dirichlet.mle(botR)$alpha
  criteria[j+5] <- -2*(my.log.like.fun(top.mle, top) + 
                         my.log.like.fun(botL.mle, botL)
                       + my.log.like.fun(botR.mle, botR))
}

## Type 4: 2, 1, 1 (12)

AA <- permutations(4, 2)
for(j in 1:12){
  aa <- AA[j, 1]
  bb <- AA[j, 2]
  b1 <- full.data[,aa]
  b2 <- 1 - b1
  b21 <- full.data[,bb]/b2
  b22 <- 1 - b21
  bot <- full.data[, -AA[j,]]/b22
  top.mle <- sirt::dirichlet.mle(cbind(b1, b2))$alpha
  mid.mle <- sirt::dirichlet.mle(cbind(b21, b22))$alpha
  bot.mle <- sirt::dirichlet.mle(bot)$alpha
  criteria[j+8] <- -2*(my.log.like.fun(top.mle, cbind(b1, b2)) + 
                         my.log.like.fun(mid.mle, cbind(b21, b22))
                       + my.log.like.fun(bot.mle, bot))
  
}

criteria
## The winner is no nesting.  Disappointment.

################################################################################
## November 11, 2021
## Visualization
################################################################################

#### Exploratory Visualization  ####

water.maze.data <- read.csv("water_maze_data.csv")

a.full.data <- acomp(water.maze.data[,2:5])
X1 <- factor(water.maze.data[,1])
opar = par(xpd=NA)
plot(a.full.data, pch=c(3,20)[X1], col=c("red", "blue")[X1], 
     cex=c(1.3, 1.3)[X1])
legend(x=0.42, y=0.4, levels(X1), pch=c(3,20), col=c("red", "blue"))



jpeg("mice_tern.jpg", width = 500, height = 500)
opar = par(xpd=NA)
plot(a.full.data, pch=c(3,20)[X1], col=c("red", "blue")[X1])
legend(x=-0.25, y=1.0, levels(X1), pch=c(3,20), col=c("red", "blue"))
dev.off()

X1 <- factor(water.maze.data[,1])
plot(water.maze.data[,2:5], pch=c(3,20)[X1], col=c("red", "blue")[X1], 
     cex=c(1.3, 1.3)[X1])


###############################################################################
## November 9, 2021
## Using Turner's Hypothesis Test
## This is a likelihood ratio test statistic without bias correction
###############################################################################

#### Turner's Test ####

water.maze.data <- read.csv("water_maze_data.csv")

## Log pbar function

log.pbar <- function(data.mat){
  log.data <- log(data.mat)
  return(apply(log.data, 2, mean))
}

## Practice with the Optim function
my.fun <- function(x){
  y <- 15-2*x - x^2
  return(y)
}

optim(-3, my.fun, method = 'Brent', lower= -8, upper =4, control = list(fnscale=-1))

## Second test function
my.fun.2 <- function(xx){
  x <- xx[1]
  y <- xx[2]
  res <- -x^2*y + y^2*4 -x^2 +2*x*y
}

optim(par=c(2, 3), my.fun.2)
## Note:  optim finds minimums not maximums by default.
## To get otpim to find the maximum use control = list(fnscale=-1)
## Use method='Brent' and give upper and lower bounds for one dimensional problems.

## Function for log-likelihood under the unrestricted parameter space.
## This is for the alternative hypothesis.

log.like.unrestricted.1 <- function(param, x1, x2){
  ppi1 <- param[1:3]
  ppi2 <- param[4:6]
  A1 <- param[7]
  A2 <- param[8]
  n1 <- nrow(x1)
  n2 <- nrow(x2)
  log.x1.bar <- apply(log(x1), 2, mean)
  log.x2.bar <- apply(log(x2), 2, mean)
  
  yy <- n1*lgamma(A1) - n1*sum(lgamma(A1*c(ppi1, 1-sum(ppi1)))) + 
                                 n1*sum((A1*c(ppi1, 1-sum(ppi1))-1)*log.x1.bar) +
        n2*lgamma(A2) - n2*sum(lgamma(A2*c(ppi2, 1-sum(ppi2)))) + 
  n2*sum((A2*c(ppi2, 1-sum(ppi2))-1)*log.x2.bar)
  return(yy)
}

## Note:  The parameter vector is the ppi vector for pop 1, followed by the 
## ppi vector for pop2, then A1 then A2.

x1 <- water.maze.data[1:7, 2:5]
x2 <- water.maze.data[8:14, 2:5]


## Note:  control = list(fnscale=-1) multiples the function by -1 to find a 
## maximum rather than a minimum.


## Without mean parameters being constrained between 0 and 1. 
## Gives poor estimates of the precision parameters
optim(par= c(rep(0.25, 6), 10, 10), 
    log.like.unrestricted.1, x1=x1, x2=x2, control = list(fnscale=-1))

## With mean parameters constrained between 0 and 1.
## Gives good estimates of the precision parameters.
optim(par= c(rep(0.25, 6), 10, 10), 
      log.like.unrestricted.1, x1=x1, x2=x2, control = list(fnscale=-1),
      method="L-BFGS-B", lower =rep(0.01, 8), upper = c(rep(1, 6), 50, 50))

sirt::dirichlet.mle(x1)
sirt::dirichlet.mle(x2)

##Note:  convergence value of 0 means convergence and 1 means no convergence.

##  Try with simulated data.

my.x1 <- gtools::rdirichlet(1000, c(10, 30, 20, 40))
my.x2 <- gtools::rdirichlet(1000, c(10, 13, 12, 15))

optim(par= c(rep(0.25, 6), 10, 10), 
      log.like.unrestricted.1, x1=my.x1, x2=my.x2, control = list(fnscale=-1),
      method="L-BFGS-B", lower =rep(0.01, 8), upper = c(rep(1, 6), 200, 200))

##  Everything looks great according to this test.  Time to Proceed.

## The restricted log likelihood that we get under the null hypothesis.
## The first three entries of param are the common mean parameters.
## The fourth param is the precision for pop1.
## The fifth  param is the precison for pop2.
log.like.null.1 <- function(param, x1, x2){
  ppi <- param[1:3]
  A1 <- param[4]
  A2 <- param[5]
  n1 <- nrow(x1)
  n2 <- nrow(x2)
  log.x1.bar <- apply(log(x1), 2, mean)
  log.x2.bar <- apply(log(x2), 2, mean)
  
  yy <- n1*lgamma(A1) - n1*sum(lgamma(A1*c(ppi, 1-sum(ppi)))) + 
    n1*sum((A1*c(ppi, 1-sum(ppi))-1)*log.x1.bar) +
    n2*lgamma(A2) - n2*sum(lgamma(A2*c(ppi, 1-sum(ppi)))) + 
    n2*sum((A2*c(ppi, 1-sum(ppi))-1)*log.x2.bar)
  return(yy)
}

optim(par= c(rep(0.25, 3), 10, 10), 
      log.like.null.1, x1=x1, x2=x2, control = list(fnscale=-1),
      method="L-BFGS-B", lower =rep(0.01, 5), upper = c(rep(1, 3), 50, 50))

## November 13, 2021
## Create LRT test statistic

null.max <- optim(par= c(rep(0.25, 3), 10, 10), 
      log.like.null.1, x1=x1, x2=x2, control = list(fnscale=-1),
      method="L-BFGS-B", lower =rep(0.01, 5), upper = c(rep(1, 3), 50, 50))

alt.max <- optim(par= c(rep(0.25, 6), 10, 10), 
      log.like.unrestricted.1, x1=x1, x2=x2, control = list(fnscale=-1),
      method="L-BFGS-B", lower =rep(0.01, 8), upper = c(rep(1, 6), 50, 50))

lr.stat.wm <- -2*(null.max$value-alt.max$value)  ## 7.222988

## The Likelihood ratio test statistic follows a chi-square distribution 
## with df = 8 parameters (alt) - 5 parameters (null) = 3 

pchisq(lr.stat.wm, 3, lower.tail = FALSE)  ## 0.06511998

#### GARBAGE IGNORE ####
###November 19, 2021 
## I wrote my bias approximation in mathematica
## Now I am rewriting them here


## Derivative of the vectorized Hessian (transposed) 3 by 9 matrix
der.vec.hess <- function(n, a1, a2, a3){
  A <- a1+a2+a3
  my.mat <- matrix( rep( psigamma(A, deriv=2), 27), nrow=3, ncol=9)
  my.mat[1, 1] <- my.mat[1, 1] - psigamma(a1, deriv=2)
  my.mat[2, 5] <- my.mat[2, 5] - psigamma(a2, deriv=2)
  my.mat[3, 9] <- my.mat[3, 9] - psigamma(a3, deriv=2)
  my.mat <- my.mat*n
  return(my.mat)
}

## Inverse Information Matrix Function
inverse.info.mat <- function(n, a1, a2, a3){
  A <- a1+a2+a3
  my.mat <- matrix( rep(trigamma(A), 9), ncol=3, nrow=3)
  my.mat[1, 1] <- my.mat[1, 1] - trigamma(a1)
  my.mat[2, 2] <- my.mat[2, 2] - trigamma(a2)
  my.mat[3, 3] <- my.mat[3, 3] - trigamma(a3)
  my.mat <- -n*my.mat
  my.mat <- matlib::Inverse(my.mat)
  return(my.mat)
}

## The Bias Function
approx.bias <- function(n, a1, a2, a3){
  result <- 0.5*inverse.info.mat(n, a1, a2, a3) %*% der.vec.hess(n, a1, a2, a3) %*%
    c(inverse.info.mat(n, a1, a2, a3))
  return(result)
}

## Test it out 
approx.bias(20, 10, 20, 30)

## Script for the Water Maze Data 

water.maze.data <- read.csv("water_maze_data.csv")

null.max <- optim(par= c(rep(0.25, 3), 10, 10), 
                  log.like.null.1, x1=x1, x2=x2, control = list(fnscale=-1),
                  method="L-BFGS-B", lower =rep(0.01, 5), upper = c(rep(1, 3), 50, 50))

alt.max <- optim(par= c(rep(0.25, 6), 10, 10), 
                 log.like.unrestricted.1, x1=x1, x2=x2, control = list(fnscale=-1),
                 method="L-BFGS-B", lower =rep(0.01, 8), upper = c(rep(1, 6), 50, 50))

## alphas for the null
null.mean <- c(null.max$par[1:3], 1-sum(null.max$par[1:3]))
null.A.1 <- null.max$par[4]
null.alpha.1 <- null.mean*null.A.1
null.A.2 <- null.max$par[5]
null.alpha.2 <- null.mean*null.A.2

null.bias.1 <- approx.bias(7, null.alpha.1)
null.bias.2 <- approx.bias(7, null.alpha.2)

## Nov. 20, 2021  

## Oops I just realized my example has four parameters, but my functions have 
## three parameters.  I have to rewrite them all.  Instead of erasing them 
## I will re-make them here.


## Derivative of the vectorized Hessian (transposed) 3 by 9 matrix
##Note:  This can be generalized to any size.

der.vec.hess <- function(n, a1, a2, a3, a4){
  A <- a1+a2+a3+a4
  my.mat <- matrix( rep( psigamma(A, deriv=2), 64), nrow=4, ncol=16)
  my.mat[1, 1] <- my.mat[1, 1] - psigamma(a1, deriv=2)
  my.mat[2, 6] <- my.mat[2, 6] - psigamma(a2, deriv=2)
  my.mat[3, 11] <- my.mat[3, 11] - psigamma(a3, deriv=2)
  my.mat[4, 16] <- my.mat[4, 16] - psigamma(a4, deriv=2)
  my.mat <- my.mat*n
  return(my.mat)
}

## Inverse Information Matrix Function
inverse.info.mat <- function(n, a1, a2, a3, a4){
  A <- a1+a2+a3+a4
  my.mat <- matrix( rep(trigamma(A), 16), ncol=4, nrow=4)
  my.mat[1, 1] <- my.mat[1, 1] - trigamma(a1)
  my.mat[2, 2] <- my.mat[2, 2] - trigamma(a2)
  my.mat[3, 3] <- my.mat[3, 3] - trigamma(a3)
  my.mat[4, 4] <- my.mat[4, 4] - trigamma(a4)
  my.mat <- -n*my.mat
  my.mat <- matlib::Inverse(my.mat)
  return(my.mat)
}

## The Bias Function
approx.bias <- function(n, alpha){
  a1 <- alpha[1]
  a2 <- alpha[2]
  a3 <- alpha[3]
  a4 <- alpha[4]
  result <- c(0.5*inverse.info.mat(n, a1, a2, a3, a4) %*% 
    der.vec.hess(n, a1, a2, a3, a4) %*%c(inverse.info.mat(n, a1, a2, a3, a4)))
  return(result)
}

## Script for the Water Maze Data 

water.maze.data <- read.csv("water_maze_data.csv")
x1 <- water.maze.data[1:7, 2:5]
x2 <- water.maze.data[8:14, 2:5]

null.max <- optim(par= c(rep(0.25, 3), 10, 10), 
                  log.like.null.1, x1=x1, x2=x2, control = list(fnscale=-1),
                  method="L-BFGS-B", lower =rep(0.01, 5), upper = c(rep(1, 3), 50, 50))

alt.max <- optim(par= c(rep(0.25, 6), 10, 10), 
                 log.like.unrestricted.1, x1=x1, x2=x2, control = list(fnscale=-1),
                 method="L-BFGS-B", lower =rep(0.01, 8), upper = c(rep(1, 6), 50, 50))

## alphas for the null
null.mean <- c(null.max$par[1:3], 1-sum(null.max$par[1:3]))
null.A.1 <- null.max$par[4]
null.alpha.1 <- null.mean*null.A.1
null.A.2 <- null.max$par[5]
null.alpha.2 <- null.mean*null.A.2

null.bias.1 <- approx.bias(7, null.alpha.1)
null.bias.2 <- approx.bias(7, null.alpha.2)

bc.null.alpha.1 <- null.alpha.1 - null.bias.1
bc.null.alpha.2 <- null.alpha.2 - null.bias.2
bc.A.1 <- sum(bc.null.alpha.1)
bc.A.2 <- sum(bc.null.alpha.2)

null.param <- c(bc.null.alpha.1[1:3]/bc.A.1, bc.A.1,  bc.A.2)

null.max <- log.like.null.1(null.param, x1=x1, x2=x2)
null.max  ##51.64592

## Alternative BC max
alt.mean.1 <- c(alt.max$par[1:3], 1-sum(alt.max$par[1:3]))
alt.A.1 <- alt.max$par[7]
alt.alpha.1 <- alt.mean.1*alt.A.1

alt.mean.2 <- c(alt.max$par[4:6], 1-sum(alt.max$par[4:6]))
alt.A.2 <- alt.max$par[8]
alt.alpha.2 <- alt.mean.2*alt.A.2

alt.bias.1 <- approx.bias(7, alt.alpha.1)
alt.bias.2 <- approx.bias(7, alt.alpha.2)

bc.alt.alpha.1 <- alt.alpha.1 - alt.bias.1
bc.alt.alpha.2 <- alt.alpha.2 - alt.bias.2
bc.A.1 <- sum(bc.alt.alpha.1)
bc.A.2 <- sum(bc.alt.alpha.2)

alt.param <- c(bc.alt.alpha.1[1:3]/bc.A.1, bc.null.alpha.2[1:3]/bc.A.2,
               bc.A.1, bc.A.2)

alt.max <- log.like.unrestricted.1(alt.param, x1=x1, x2=x2)
alt.max

lr.stat.wm.bc <- -2*(null.max-alt.max)  ## 7.222988

## The Likelihood ratio test statistic follows a chi-square distribution 
## with df = 8 parameters (alt) - 5 parameters (null) = 3 

pchisq(lr.stat.wm, 3, lower.tail = FALSE)  ## 0.06511998

## I think that all of the above is Garbage!


#### Simulating Nested Dirichlet ####

## It is now December 4th 2021

set.seed(1990)
level.one <- gtools::rdirichlet(1000, c(8.1, 11.2))
level.two.left <- gtools::rdirichlet(1000, c(11.6, 10.3))
level.two.right <- gtools::rdirichlet(1000, c(5.6, 9.2))

AQ1 <- level.one[, 1]*level.two.left[,1]
OQ <- level.one[, 1]*level.two.left[,2]
AQ2 <- level.one[, 2]*level.two.right[,1]
TQ <- level.one[, 2]*level.two.right[,2]

sim.wm.data <- data.frame(TQ, AQ1, OQ, AQ2)
head(sim.wm.data)

##Correlation matrix for simulated data.
cor(sim.wm.data)

observed.data <- as.data.frame(full.data)

## Creating a Ternary diagram of simulated data and observed data.

Group <- factor(c(rep("observed", 14), rep("simulated", 1000)))
combined.data <- rbind(observed.data, sim.wm.data)

a.combined.data <- acomp(combined.data)



jpeg("ObsVSim.jpg", width = 500, height = 500)
opar = par(xpd=NA)
plot(a.combined.data, pch=c(20,20)[Group], 
     col=c(rgb(red=1, green=0, blue=0, alpha=1), 
           rgb(red=0, green=0, blue=1, alpha=0.1))[Group], 
     cex=c(1.3, 0.7)[Group])
legend(x=0, y=1.0, levels(Group), pch=c(20,20), col=c("red", "blue"))
dev.off()

#### December 5th:  Nested Dirichlet Test ####
## I am going to rewrite my likelihoods for just two variables at each branch.

##Log Likelihood under the null with two branches.
## Parameters are the mean vectors.
## Recall:  You need k - 1 values in the mean vector.
## x1 and x2 are the sample data.

log.like.null.2 <- function(param, x1, x2){
  ppi <- param[1]
  A1 <- param[2]
  A2 <- param[3]
  n1 <- nrow(x1)
  n2 <- nrow(x2)
  log.x1.bar <- apply(log(x1), 2, mean)
  log.x2.bar <- apply(log(x2), 2, mean)
  
  yy <- n1*lgamma(A1) - n1*sum(lgamma(A1*c(ppi, 1-ppi))) + 
    n1*sum((A1*c(ppi, 1-ppi)-1)*log.x1.bar) +
    n2*lgamma(A2) - n2*sum(lgamma(A2*c(ppi, 1-ppi))) + 
    n2*sum((A2*c(ppi, 1-ppi)-1)*log.x2.bar)
  return(yy)
}


## Log likelihood under the alternative hypothesis.
## ppi1 an ppi2 are the mean vectors.  
## X1 and X2 are the sample data.

log.like.unrestricted.2 <- function(param, x1, x2){
  ppi1 <- param[1]
  ppi2 <- param[2]
  A1 <- param[3]
  A2 <- param[4]
  n1 <- nrow(x1)
  n2 <- nrow(x2)
  log.x1.bar <- apply(log(x1), 2, mean)
  log.x2.bar <- apply(log(x2), 2, mean)
  
  yy <- n1*lgamma(A1) - n1*sum(lgamma(A1*c(ppi1, 1-ppi1))) + 
    n1*sum((A1*c(ppi1, 1-ppi1)-1)*log.x1.bar) +
    n2*lgamma(A2) - n2*sum(lgamma(A2*c(ppi2, 1-ppi2))) + 
    n2*sum((A2*c(ppi2, 1-ppi2)-1)*log.x2.bar)
  return(yy)
}

## Data 
water.maze.data <- read.csv("water_maze_data.csv")
y1 <- water.maze.data[1:7, 2:5]
y2 <- water.maze.data[8:14, 2:5]

## N1 sum of AQ1 and OQ
## N2 sum of AQ2 and TQ
x1.data <- list()
x2.data <- list()

## Level 1
## N1 sum of AQ1 and OQ
## N2 sum of AQ2 and TQ
x1.data[[1]] <- cbind(y1[, 2]+y1[,3], y1[,1]+y1[,4])
x2.data[[1]] <- cbind(y2[,2]+y2[,3], y2[,1]+y2[,4])

## Level 2 Left: AQ1 and OQ

x1.data[[2]] <- cbind(y1[,2]/(y1[, 2]+y1[,3]), y1[,3]/(y1[, 2]+y1[,3]))
x2.data[[2]] <- cbind(y2[,2]/(y2[, 2]+y2[,3]), y2[,3]/(y2[, 2]+y2[,3]))

## Level 3 Right:  AQ2 and TQ

x1.data[[3]] <- cbind(y1[,4]/(y1[,1]+y1[,4]), y1[,1]/(y1[,1]+y1[,4]))
x2.data[[3]] <- cbind(y2[,4]/(y2[,1]+y2[,4]), y2[,1]/(y2[,1]+y2[,4]))


## Calculating the three LRT test staistics.
## Empty dataframe

LRT.df <- data.frame(matrix(ncol = 3, nrow = 0))
#provide column names
colnames(LRT.df) <- c('null.max', 'alt.max', 'lr.stat')

## Calculate the three LRT test statistics.

for( i in 1:3){
  x1 <- x1.data[[i]]
  x2 <- x2.data[[i]]
  null.max <- optim(par= c(0.6, 15, 15), 
                  log.like.null.2, x1=x1, x2=x2, control = list(fnscale=-1),
                  method="L-BFGS-B", lower =rep(0.01, 3), upper = c(1, 60, 60))

  alt.max <- optim(par= c(0.6, 0.6, 15, 15), 
                 log.like.unrestricted.2, x1=x1, x2=x2, control = list(fnscale=-1),
                 method="L-BFGS-B", lower =rep(0.01, 4), upper = c(1, 1, 60, 60))

  lr.stat <- -2*(null.max$value - alt.max$value)
  
  LRT.df[i,] <- c(null.max$value, alt.max$value, lr.stat)
}
LRT.df

(lambda.overall <- colSums(LRT.df)[3])

## The Likelihood ratio test statistic follows a chi-square distribution 
## with df = 3

pchisq(lambda.overall, 3, lower.tail = FALSE)  ## 0.1053




#### August 16, 2022 ####
## Check that the five possible trees give the correct correlation structure.
## Calculate the MLEs for the five models.
## Find the log-likelihood of the five models.
## Select the one with the max- log-likelihood.

## Model A

S1.A <- rowSums(full.data[,2:3])
S2.A <- full.data[,1]+full.data[,4]
L1.A <- cbind(S1.A, S2.A)
L2.A <- cbind(full.data[,2]/S1.A, full.data[,3]/S1.A)
L3.A <- cbind(full.data[,1]/S2.A, full.data[,4]/S2.A)
alp1.A <- sirt::dirichlet.mle(L1.A)$alpha
alp2.A <- sirt::dirichlet.mle(L2.A)$alpha
alp3.A <-sirt::dirichlet.mle(L3.A)$alpha
sim1.A <- rdirichlet(1000, alp1.A)
sim2.A <- rdirichlet(1000, alp2.A)
sim3.A <- rdirichlet(1000, alp3.A)
sim.A <- cbind(sim1.A[,1]*sim2.A[,1], sim1.A[,1]*sim2.A[,2],
               sim1.A[,2]*sim3.A[,1], sim1.A[,2]*sim3.A[,2])
sim.A <- cbind(sim.A[,3], sim.A[,1], sim.A[,2], sim.A[,4])
cor(sim.A)
cor(full.data)


## Model B
S1.B <- full.data[,2]+full.data[,3]
L1.B <- cbind(S1.B, full.data[,4], full.data[,1])
L2.B <- cbind(full.data[,2]/S1.B, full.data[,3]/S1.B)
alp1.B <- sirt::dirichlet.mle(L1.B)$alpha
alp2.B <- sirt::dirichlet.mle(L2.B)$alpha
sim1.B <- rdirichlet(1000, alp1.B)
sim2.B <- rdirichlet(1000, alp2.B)
sim.B <- cbind(sim1.B[,3], sim1.B[,1]*sim2.B[,1], sim1.B[,1]*sim2.B[,2], sim1.B[,2]) 
cor(sim.B)
cor(full.data)

## Model C
S1.C <- rowSums(full.data[,c(1, 2, 3)])
S2.C <- rowSums(full.data[,c(2, 3)])
L1.C <- cbind(full.data[,4], S1.C)
L2.C <- cbind(full.data[,1]/S1.C, S2.C/S1.C)
L3.C <- cbind(full.data[,2]/S2.C, full.data[,3]/S2.C)
alp1.C <- sirt::dirichlet.mle(L1.C)$alpha
alp2.C <- sirt::dirichlet.mle(L2.C)$alpha
alp3.C <- sirt::dirichlet.mle(L3.C)$alpha
sim1.C <- rdirichlet(1000, alp1.C)
sim2.C <- rdirichlet(1000, alp2.C)
sim3.C <- rdirichlet(1000, alp3.C)
sim.C <- cbind(sim1.C[,2]*sim2.C[,1],
               sim1.C[,2]*sim2.C[,2]*sim3.C[,1],
               sim1.C[,2]*sim2.C[,2]*sim3.C[,2], 
               sim1.C[,1])
cor(sim.C)
cor(full.data)

##Model D
S1.D <- rowSums(full.data[,c(1, 2, 3)])
S2.D <- rowSums(full.data[,c(1, 2)])
L1.D <- cbind(full.data[,4], S1.D)
L2.D <- cbind(full.data[,3]/S1.D, S2.D/S1.D)
L3.D <- cbind(full.data[,1]/S2.D, full.data[,2]/S2.D)
alp1.D <- sirt::dirichlet.mle(L1.D)$alpha
alp2.D <- sirt::dirichlet.mle(L2.D)$alpha
alp3.D <- sirt::dirichlet.mle(L3.D)$alpha
sim1.D <- rdirichlet(1000, alp1.D)
sim2.D <- rdirichlet(1000, alp2.D)
sim3.D <- rdirichlet(1000, alp3.D)
sim.D <- cbind(sim1.D[,2]*sim2.D[,2]*sim3.D[,1],
               sim1.D[,2]*sim2.D[,2]*sim3.D[,2], 
               sim1.D[,2]*sim2.D[,1],
               sim1.D[,1])
cor(sim.D)
cor(full.data)
